{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7eiDWcM_MC3H"
   },
   "source": [
    "# <font color='red'>Implement SGD Classifier with Logloss and L2 regularization Using SGD without using sklearn</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfe2NTQtLq11"
   },
   "source": [
    "**There will be some functions that start with the word \"grader\" ex: grader_weights(), grader_sigmoid(), grader_logloss() etc, you should not change those function definition.<br><br>Every Grader function has to return True.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fk5DSPCLxqT-"
   },
   "source": [
    "<font color='red'> Importing packages</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "42Et8BKIxnsp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NpSk3WQBx7TQ"
   },
   "source": [
    "<font color='red'>Creating custom dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BsMp0oWzx6dv"
   },
   "outputs": [],
   "source": [
    "# please don't change random_state\n",
    "X, y = make_classification(n_samples=50000, n_features=15, n_informative=10, n_redundant=5,\n",
    "                           n_classes=2, weights=[0.7], class_sep=0.7, random_state=15)\n",
    "# make_classification is used to create custom dataset \n",
    "# Please check this link (https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L8W2fg1cyGdX",
    "outputId": "029d4c84-03b2-4143-a04c-34ff49c88890"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 15), (50000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.17133535, -1.00849691,  0.40726112, -2.05334509, -1.37381592,\n",
       "        -2.99724545,  0.7787227 ,  0.87207405, -2.17362041,  1.22938588,\n",
       "         0.21266735, -2.21599818, -1.8801447 , -0.61688062, -0.68442615],\n",
       "       [-1.73587152, -2.57009747,  0.28330916,  0.8561389 ,  0.44892601,\n",
       "        -0.03924508,  3.48407244,  2.65068639,  5.46479732, -0.29455059,\n",
       "        -8.44133995,  2.67957026, -4.18507993,  0.55904337,  3.31251558]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y.count_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x99RWCgpqNHw"
   },
   "source": [
    "<font color='red'>Splitting data into train and test </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Kh4dBfVyJMP"
   },
   "outputs": [],
   "source": [
    "#please don't change random state\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0DR_YMBsyOci",
    "outputId": "732014d9-1731-4d3f-918f-a9f5255ee149"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37500, 15), (37500,), (12500, 15), (12500,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.57349184, -0.19015688, -0.06584143, -0.86990562, -2.80927706,\n",
       "        -1.43345052,  0.35862361,  0.24627836, -2.25803168, -0.87761289,\n",
       "         2.31023199, -0.3484947 , -2.2575668 , -1.93628665,  1.65242231],\n",
       "       [ 1.827818  , -0.45810992,  0.47407375, -2.17856544, -1.16453085,\n",
       "        -0.59906384,  2.24400146,  0.2664526 , -1.59252721, -2.3705834 ,\n",
       "        -1.14068014, -1.83108915, -0.32123197,  0.31287131, -1.494433  ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BW4OHswfqjHR"
   },
   "source": [
    "# <font color='red' size=5>SGD classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "3HpvTwDHyQQy",
    "outputId": "5729f08c-079a-4b17-bf51-f9aeb5abb13b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=25, verbose=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alpha : float\n",
    "# Constant that multiplies the regularization term. \n",
    "\n",
    "# eta0 : double\n",
    "# The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules.\n",
    "\n",
    "clf = linear_model.SGDClassifier(eta0=0.0001, alpha=0.0001, loss='log', random_state=25, penalty='l2', tol=1e-3, verbose=2, learning_rate='constant')\n",
    "clf\n",
    "# Please check this documentation (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "YYaVyQ2lyXcr",
    "outputId": "dc0bf840-b37e-4552-e513-84b64f6c64c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 0.76, NNZs: 15, Bias: -0.314189, T: 37500, Avg. loss: 0.456802\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 0.91, NNZs: 15, Bias: -0.473167, T: 75000, Avg. loss: 0.394854\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 0.98, NNZs: 15, Bias: -0.582247, T: 112500, Avg. loss: 0.385592\n",
      "Total training time: 0.05 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.02, NNZs: 15, Bias: -0.660004, T: 150000, Avg. loss: 0.382079\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.04, NNZs: 15, Bias: -0.719364, T: 187500, Avg. loss: 0.380468\n",
      "Total training time: 0.06 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 1.05, NNZs: 15, Bias: -0.762588, T: 225000, Avg. loss: 0.379512\n",
      "Total training time: 0.07 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 1.06, NNZs: 15, Bias: -0.794983, T: 262500, Avg. loss: 0.379125\n",
      "Total training time: 0.09 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 1.07, NNZs: 15, Bias: -0.819529, T: 300000, Avg. loss: 0.378776\n",
      "Total training time: 0.10 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 1.07, NNZs: 15, Bias: -0.836210, T: 337500, Avg. loss: 0.378673\n",
      "Total training time: 0.12 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 1.08, NNZs: 15, Bias: -0.852928, T: 375000, Avg. loss: 0.378612\n",
      "Total training time: 0.12 seconds.\n",
      "Convergence after 10 epochs took 0.12 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(eta0=0.0001, learning_rate='constant', loss='log',\n",
       "              random_state=25, verbose=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X=X_train, y=y_train) # fitting our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "EAfkVI6GyaRO",
    "outputId": "bc88f920-6531-4106-9b4c-4dabb6d72b47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.42128784,  0.18704563, -0.14732383,  0.34374215, -0.2108936 ,\n",
       "          0.56611196, -0.44669126, -0.09835731,  0.21042399,  0.17331533,\n",
       "          0.1986096 ,  0.00613826, -0.07452533,  0.33716418,  0.02584266]]),\n",
       " (1, 15),\n",
       " array([-0.85292835]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.coef_.shape, clf.intercept_\n",
    "#clf.coef_ will return the weights\n",
    "#clf.coef_.shape will return the shape of weights\n",
    "#clf.intercept_ will return the intercept term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-CcGTKgsMrY"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n",
    "## <font color='red' size=5> Implement Logistic Regression with L2 regularization Using SGD: without using sklearn </font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W1_8bdzitDlM"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "1.  We will be giving you some functions, please write code in that functions only.\n",
    "\n",
    "2.  After every function, we will be giving you expected output, please make sure that you get that output. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zU2Y3-FQuJ3z"
   },
   "source": [
    "\n",
    "<br>\n",
    "\n",
    "* Initialize the weight_vector and intercept term to zeros (Write your code in <font color='blue'>def initialize_weights()</font>)\n",
    "\n",
    "* Create a loss function (Write your code in <font color='blue'>def logloss()</font>) \n",
    "\n",
    " $log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$\n",
    "- for each epoch:\n",
    "\n",
    "    - for each batch of data points in train: (keep batch size=1)\n",
    "\n",
    "        - calculate the gradient of loss function w.r.t each weight in weight vector (write your code in <font color='blue'>def gradient_dw()</font>)\n",
    "\n",
    "        $dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)})$ <br>\n",
    "\n",
    "        - Calculate the gradient of the intercept (write your code in <font color='blue'> def gradient_db()</font>) <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>check this</a>\n",
    "\n",
    "           $ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t}))$\n",
    "\n",
    "        - Update weights and intercept (check the equation number 32 in the above mentioned <a href='https://drive.google.com/file/d/1nQ08-XY4zvOLzRX-lGf8EYB5arb7-m1H/view?usp=sharing'>pdf</a>): <br>\n",
    "        $w^{(t+1)}← w^{(t)}+α(dw^{(t)}) $<br>\n",
    "\n",
    "        $b^{(t+1)}←b^{(t)}+α(db^{(t)}) $\n",
    "    - calculate the log loss for train and test with the updated weights (you can check the python assignment 10th question)\n",
    "    - And if you wish, you can compare the previous loss and the current loss, if it is not updating, then\n",
    "        you can stop the training\n",
    "    - append this loss in the list ( this will be used to see how loss is changing for each epoch after the training is over )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZR_HgjgS_wKu"
   },
   "source": [
    "<font color='blue'>Initialize weights </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GecwYV9fsKZ9"
   },
   "outputs": [],
   "source": [
    "def initialize_weights(dim):\n",
    "    ''' In this function, we will initialize our weights and bias'''\n",
    "    #initialize the weights to zeros array of (1,dim) dimensions\n",
    "    #you use zeros_like function to initialize zero, check this link https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros_like.html\n",
    "    #initialize bias to zero\n",
    "    w=np.zeros_like(dim)\n",
    "    b=np.array([0])\n",
    "\n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A7I6uWBRsKc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.57349184 -0.19015688 -0.06584143 -0.86990562 -2.80927706 -1.43345052\n",
      "  0.35862361  0.24627836 -2.25803168 -0.87761289  2.31023199 -0.3484947\n",
      " -2.2575668  -1.93628665  1.65242231]\n",
      "w = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "b = [0]\n"
     ]
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "print(dim)\n",
    "w,b = initialize_weights(dim)\n",
    "print('w =',(w))\n",
    "print('b =',str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4MI5SAjP9ofN"
   },
   "source": [
    "<font color='cyan'>Grader function - 1 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pv1llH429wG5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim=X_train[0] \n",
    "w,b = initialize_weights(dim)\n",
    "def grader_weights(w,b):\n",
    "  assert((len(w)==len(dim)) and b==0 and np.sum(w)==0.0)\n",
    "  return True\n",
    "grader_weights(w,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QN83oMWy_5rv"
   },
   "source": [
    "<font color='blue'>Compute sigmoid </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPv4NJuxABgs"
   },
   "source": [
    "$sigmoid(z)= 1/(1+exp(-z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nAfmQF47_Sd6"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def sigmoid(z):\n",
    "    ''' In this function, we will return sigmoid of z'''\n",
    "    # compute sigmoid(z) and return\n",
    "    sig=1/(1+np.exp(-z))\n",
    "\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9YrGDwg3Ae4m"
   },
   "source": [
    "<font color='cyan'>Grader function - 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_JASp_NAfK_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_sigmoid(z):\n",
    "  val=sigmoid(z)\n",
    "  assert(val==0.8807970779778823)\n",
    "  return True\n",
    "grader_sigmoid(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gS7JXbcrBOFF"
   },
   "source": [
    "<font color='blue'> Compute loss </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lfEiS22zBVYy"
   },
   "source": [
    "$log loss = -1*\\frac{1}{n}\\Sigma_{for each Yt,Y_{pred}}(Ytlog10(Y_{pred})+(1-Yt)log10(1-Y_{pred}))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VaFDgsp3sKi6"
   },
   "outputs": [],
   "source": [
    "def logloss(y_true,y_pred):\n",
    "    '''In this function, we will compute log loss '''\n",
    "    loss=0\n",
    "    for i in range(len(y_true)):\n",
    "        loss+=(y_true[i]*np.log10(y_pred[i])+(1-y_true[i])*np.log10(1-y_pred[i]))\n",
    "    loss=(-1/len(y_true))*loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zs1BTXVSClBt"
   },
   "source": [
    "<font color='cyan'>Grader function - 3 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzttjvBFCuQ5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_logloss(true,pred):\n",
    "  loss=logloss(true,pred)\n",
    "  assert(loss==0.07644900402910389)\n",
    "  return True\n",
    "true=[1,1,0,1,0]\n",
    "pred=[0.9,0.8,0.1,0.8,0.2]\n",
    "grader_logloss(true,pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tQabIadLCBAB"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to  'w' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTMxiYKaCQgd"
   },
   "source": [
    "$dw^{(t)} = x_n(y_n − σ((w^{(t)})^{T} x_n+b^{t}))- \\frac{λ}{N}w^{(t)}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.17133535, -1.00849691,  0.40726112, -2.05334509, -1.37381592,\n",
       "       -2.99724545,  0.7787227 ,  0.87207405, -2.17362041,  1.22938588,\n",
       "        0.21266735, -2.21599818, -1.8801447 , -0.61688062, -0.68442615])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMVikyuFsKo5"
   },
   "outputs": [],
   "source": [
    "def gradient_dw(x,y,w,b,alpha,N):\n",
    "    '''In this function, we will compute the gardient w.r.to w '''\n",
    "    \n",
    "    dw = x*(y-sigmoid((np.dot(w,x)+b))-((alpha/N)*w))\n",
    "\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15,)\n",
      "(15,)\n"
     ]
    }
   ],
   "source": [
    "a=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "b=np.zeros_like(a)\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 15)\n"
     ]
    }
   ],
   "source": [
    "b=b.reshape(1,15)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(b,a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUFLNqL_GER9"
   },
   "source": [
    "<font color='cyan'>Grader function - 4 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WI3xD8ctGEnJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_dw(x,y,w,b,alpha,N):\n",
    "  grad_dw=gradient_dw(x,y,w,b,alpha,N)\n",
    "  #print(grad_dw)\n",
    "  #print(np.sum(grad_dw))\n",
    "  assert(np.sum(grad_dw)==2.613689585)\n",
    "  return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w,grad_b=initialize_weights(grad_x)\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_dw(grad_x,grad_y,grad_w,grad_b,alpha,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LE8g84_GI62n"
   },
   "source": [
    "<font color='blue'>Compute gradient w.r.to 'b' </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fHvTYZzZJJ_N"
   },
   "source": [
    "$ db^{(t)} = y_n- σ((w^{(t)})^{T} x_n+b^{t})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0nUf2ft4EZp8"
   },
   "outputs": [],
   "source": [
    " def gradient_db(x,y,w,b):\n",
    "    '''In this function, we will compute gradient w.r.to b '''\n",
    "    #print(x.shape)\n",
    "    #print(w.shape)\n",
    "    db=y-sigmoid(np.dot(w,x)+b)\n",
    "\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbcBzufVG6qk"
   },
   "source": [
    "<font color='cyan'>Grader function - 5 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TfFDKmscG5qZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grader_db(x,y,w,b):\n",
    "  grad_db=gradient_db(x,y,w,b)\n",
    "  #print(grad_db)\n",
    "  assert(grad_db==-0.5)\n",
    "  return True\n",
    "grad_x=np.array([-2.07864835,  3.31604252, -0.79104357, -3.87045546, -1.14783286,\n",
    "       -2.81434437, -0.86771071, -0.04073287,  0.84827878,  1.99451725,\n",
    "        3.67152472,  0.01451875,  2.01062888,  0.07373904, -5.54586092])\n",
    "grad_y=0\n",
    "grad_w,grad_b=initialize_weights(grad_x)\n",
    "alpha=0.0001\n",
    "N=len(X_train)\n",
    "grader_db(grad_x,grad_y,grad_w,grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TCK0jY_EOvyU"
   },
   "source": [
    "<font color='blue'> Implementing logistic regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ab=np.array([1,2,3,3,4,5,6,7])\n",
    "# print(ab)\n",
    "# print(ab.shape)\n",
    "# print(ab.reshape(1,8))\n",
    "# print(ab.reshape(1,8).shape)\n",
    "# print(ab.shape)\n",
    "# print(ab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmAdc5ejEZ25"
   },
   "outputs": [],
   "source": [
    "def train(X_train,y_train,X_test,y_test,epochs,alpha,eta0):\n",
    "    ''' In this function, we will implement logistic regression'''\n",
    "    #Here eta0 is learning rate\n",
    "    #implement the code as follows\n",
    "    # initalize the weights (call the initialize_weights(X_train[0]) function)\n",
    "    # for every epoch\n",
    "        # for every data point(X_train,y_train)\n",
    "           #compute gradient w.r.to w (call the gradient_dw() function)\n",
    "           #compute gradient w.r.to b (call the gradient_db() function)\n",
    "           #update w, b\n",
    "        # predict the output of x_train[for all data points in X_train] using w,b\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the train loss values in a list\n",
    "        # predict the output of x_test[for all data points in X_test] using w,b\n",
    "        #compute the loss between predicted and actual values (call the loss function)\n",
    "        # store all the test loss values in a list\n",
    "        # you can also compare previous loss and current loss, if loss is not updating then stop the process and return w,b\n",
    "    w,b=initialize_weights(X_train[0])\n",
    "    def logistic_pred(xa,w,b):\n",
    "        y_log_preds=[]\n",
    "        for i in range(len(xa)):\n",
    "            y_log_preds.append(sigmoid(np.dot(w,xa[i])+b))\n",
    "            \n",
    "        return y_log_preds\n",
    "\n",
    "\n",
    "    #w1=w.reshape(1,15)\n",
    "    train_losses=[]\n",
    "    test_losses=[]\n",
    "    #k=0\n",
    "    #ab=True\n",
    "    recent=0\n",
    "    #while ab:\n",
    "    for i in tqdm(range(epochs)):\n",
    "        for j in range(len(X_train)):\n",
    "            dw=gradient_dw(X[j],y[j],w,b,alpha,len(X_train))#finding the gradient w.r.t w\n",
    "            db=gradient_db(X[j],y[j],w,b) #finding gradient w.r.t b\n",
    "            w=w+(eta0*dw) #adjusting the w and b\n",
    "            b=b+(eta0*db)\n",
    "            \n",
    "        y_train_pred=logistic_pred(X_train,w,b) #predicting using custom implemented SGDC\n",
    "        train_losses.append(logloss(y_train,y_train_pred)) #finding the logloss for every epoch\n",
    "        \n",
    "        y_test_pred=logistic_pred(X_test,w,b)\n",
    "        test_losses.append(logloss(y_test,y_test_pred))\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "    return w,b,train_losses,test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sUquz7LFEZ6E"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [01:49<00:00,  2.20s/it]\n"
     ]
    }
   ],
   "source": [
    "alpha=0.0001\n",
    "eta0=0.0001\n",
    "N=len(X_train)\n",
    "epochs=50\n",
    "w,b,train_losses,test_losses=train(X_train,y_train,X_test,y_test,epochs,alpha,eta0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t1=y_train_pred[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.42830566,  0.18826257, -0.14817913,  0.34454315, -0.21755028,\n",
       "        0.57067343, -0.44869264, -0.08663945,  0.22805378,  0.19110932,\n",
       "        0.1906159 , -0.00653245, -0.08568428,  0.34529367,  0.02427341])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.89090588])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l4Zf_wPARlwY"
   },
   "source": [
    "<font color='red'>Goal of assignment</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l3eF_VSPSH2z"
   },
   "source": [
    "Compare your implementation and SGDClassifier's the weights and intercept, make sure they are as close as possible i.e difference should be in terms of 10^-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nx8Rs9rfEZ1R"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.00701782,  0.00121694, -0.00085531,  0.00080099, -0.00665667,\n",
       "          0.00456147, -0.00200137,  0.01171786,  0.01762979,  0.01779399,\n",
       "         -0.0079937 , -0.01267071, -0.01115895,  0.00812949, -0.00156924]]),\n",
       " array([-0.03797753]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are the results we got after we implemented sgd and found the optimal weights and intercept\n",
    "w-clf.coef_, b-clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "230YbSgNSUrQ"
   },
   "source": [
    "<font color='blue'>Plot epoch number vs train , test loss </font>\n",
    "\n",
    "* epoch number on X-axis\n",
    "* loss on Y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "print(len(list(range(50))))\n",
    "print(len(train_losses))\n",
    "print(len(test_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 5 losses:  [array([0.16435962]), array([0.16435962]), array([0.16435962]), array([0.16435962]), array([0.16435962])]\n"
     ]
    }
   ],
   "source": [
    "print(\"last 5 losses: \",train_losses[45:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True]\n"
     ]
    }
   ],
   "source": [
    "print(train_losses[48]-train_losses[49]<=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1O6GrRt7UeCJ"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(list(range(0,50)),train_losses,s=10,label='train_losses',color='green')\n",
    "plt.scatter(list(range(0,50)),test_losses,s=10,label='test_losses',color='blue')\n",
    "plt.plot(list(range(0,50)),train_losses,color='green')\n",
    "plt.plot(list(range(0,50)),test_losses,color='blue')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FUN8puFoEZtU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9543200000000001\n",
      "0.95088\n"
     ]
    }
   ],
   "source": [
    "def pred(w,b, X):\n",
    "    N = len(X)\n",
    "    predict = []\n",
    "    for i in range(N):\n",
    "        z=np.dot(w,X[i])+b\n",
    "        if sigmoid(z) >= 0.5: # sigmoid(w,x,b) returns 1/(1+exp(-(dot(x,w)+b)))\n",
    "            predict.append(1)\n",
    "        else:\n",
    "            predict.append(0)\n",
    "    return np.array(predict)\n",
    "print(1-np.sum(y_train - pred(w,b,X_train))/len(X_train))\n",
    "print(1-np.sum(y_test  - pred(w,b,X_test))/len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8310133333333334\n",
      "0.83376\n"
     ]
    }
   ],
   "source": [
    "from  sklearn.metrics import accuracy_score\n",
    "train_predict=pred(w,b,X_train)\n",
    "test_predict=pred(w,b,X_test)\n",
    "print(accuracy_score(y_train,train_predict))\n",
    "print(accuracy_score(y_test,test_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RMokBfs3-2PY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
